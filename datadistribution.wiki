== How to Write a program that uses a PaRSEC-Enabled Operation ==

This section is written using the very simple Round Time Trip
benchmark, located in {{{<SOURCE_DIR>/tests/pingpong/}}}. Using the JDF Translator tool
(parsecpp, see end of [[writejdf|How to write a PaRSEC enabled operation]]), 
you have built a parsec object generator function. This
function comes into two files: {{{<SOURCE_DIR>/tests/pingpong/rtt.jdf}}} will
generate {{{<BUILD_DIR>/tests/pingpong/rtt.h}}} and {{{<BUILD_DIR>/tests/pingpong/rtt.c}}}.

=== Overlook ===

If you look at {{{rtt.h}}}, you will find the following:
{{{
#ifndef _rtt_h_
#define _rtt_h_
#include "parsec.h"
#include "parsec/constants.h"
#include "parsec/data_distribution.h"
#include "parsec/data_internal.h"
#include "parsec/debug.h"
#include "parsec/ayudame.h"
#include "parsec/devices/device.h"
#include "parsec/interfaces/interface.h"
#include <assert.h>

BEGIN_C_DECLS

#define PARSEC_rtt_DEFAULT_ARENA    0
#define PARSEC_rtt_ARENA_INDEX_MIN 1

typedef struct parsec_rtt_handle_s {
    parsec_handle_t super;
    /* The list of globals */
    parsec_ddesc_t *_g_A /* data A */ ;
    int _g_NT;
    int _g_WS;
    /* The array of datatypes (DEFAULT and co.) */
    parsec_arena_t **arenas;
    int arenas_size;
} parsec_rtt_handle_t;

extern parsec_rtt_handle_t *parsec_rtt_new(parsec_ddesc_t * _g_A /* data A */ , int _g_NT, int _g_WS);

typedef struct __parsec_rtt_PING_assignment_s {
    assignment_t k;
    assignment_t reserved[MAX_LOCAL_COUNT - 1];
} __parsec_rtt_PING_assignment_t;

typedef struct __parsec_rtt_PING_data_s {
    parsec_data_pair_t _f_T;
    parsec_data_pair_t unused[MAX_LOCAL_COUNT - 1];
} __parsec_rtt_PING_data_t;

typedef struct __parsec_rtt_PING_task_s {
    PARSEC_MINIMAL_EXECUTION_CONTEXT
#if defined(PARSEC_PROF_TRACE)
    parsec_profile_ddesc_info_t prof_info;
#endif				/* defined(PARSEC_PROF_TRACE) */
    struct __parsec_rtt_PING_assignment_s locals;
#if defined(PINS_ENABLE)
    int creator_core;
    int victim_core;
#endif				/* defined(PINS_ENABLE) */
#if defined(PARSEC_SIM)
    int sim_exec_date;
#endif
    struct __parsec_rtt_PING_data_s data;
} __parsec_rtt_PING_task_t;


END_C_DECLS

#endif				/* _rtt_h_ */
}}}

{{{parsec_rtt_new}}} is the generator function, {{{parsec_rtt_destroy}}},
the cleanup function. In the {{{parsec_rtt_handle_t}}} structure, you
can find the data {{{A}}}, under the name {{{_g_A}}}, that is defined
in the code of {{{rtt.jdf}}}, the globals {{{NT}}} and {{{WS}}}, under
the names {{{_g_NT}}} and {{{_g_WS}}} that are also defined
in {{{rtt.jdf}}}}, and fields and variables related to /arenas/ (the
{{{arenas}}} array of the structure, the {{{arenas_size}}} counter,
and the defines {{{PARSEC_rtt_DEFAULT_ARENA}}} and
{{{PARSEC_rtt_ARENA_INDEX_MIN}}}).

In order to move data between nodes during the execution, PARSEC needs
to know what is the shape of the data, how it is distributed among the
nodes, and will need to allocate temporary memory space to store such
data elements when moving them along.

Data shape (the fact that a data element is for example a NT x MT
matrix of doubles) is defined using the powerful MPI Datatype
representation. Each data type must thus be assigned an MPI data type.
Arenas are the objects internally managed by PaRSEC to represent these
types in a more abstract way. Notably, arenas define how to allocate
temporary memory for storing such data elements.

User data fields ({{{_g_A}}}) and corresponding parameters to the
{{{parsec_rtt_new}}} function are of type {{{dague_ddesc_t}}}.
These objects represent data distributions. Each
data that is referenced in a JDF is thus a {{{parsec_ddesc_t *}}}, or a
pointer to a structure that begins with a {{{parsec_ddesc_t}}}. We will see
later how the data distribution is expressed using these objects.

=== Wrapper ===

The first thing that we advise you do is to create a set of
wrapper for these new / destroy functions to fillup the missing
elements and cleanup the invokation. That is what the files
{{{<SOURCE_DIR>/tests/pingpong/rtt_wrapper.c}}},
{{{<SOURCE_DIR>/tests/pingpong/rtt_wrapper.h}}} do:

{{{
#include "parsec.h"
#include "parsec/data_distribution.h"
#include "parsec/arena.h"

#if defined(PARSEC_HAVE_MPI)
#include <mpi.h>
static MPI_Datatype block;
#endif
#include <stdio.h>
}}}

If using /MPI/, {{{mpi.h}}} must be included. {{{parsec_config.h}}} defines a macro
{{{PARSEC_HAVE_MPI}}} if and only if parsec has been compiled with /MPI/, so this
macro can be tested to see whether it is needed to include {{{mpi.h}}} or
not. We also define a static to hold the /MPI/ definition of the datatype
PaRSEC is going to use to move the {{{A}}} data along.

{{{
#include "rtt.h"
#include "rtt_wrapper.h"

/**
 * @param [IN] A    the data, already distributed and allocated
 * @param [IN] size size of each local data element
 * @param [IN] nb   number of iterations
 *
 * @return the parsec object to schedule.
 */
parsec_handle_t *rtt_new(parsec_ddesc_t *A, int size, int nb)
{
    int worldsize;
    parsec_rtt_handle_t *o = NULL;
#if defined(PARSEC_HAVE_MPI)
    MPI_Comm_size(MPI_COMM_WORLD, &worldsize);
#else
    worldsize = 1;
#endif

    if( nb <= 0 || size <= 0 ) {
        fprintf(stderr, "To work, RTT must do at least one round time trip of at least one byte\n");
        return (parsec_handle_t*)o;
    }

    o = parsec_rtt_new(A, nb, worldsize);
}}}

The wrapper begins by doing some safety checking, then it calls the
generator function (whose definition is in the generated file {{{rtt.c}}}),
o create a ping pong DAG that will iterate on {{{A}}} {{{nb}}} times.

{{{
#if defined(PARSEC_HAVE_MPI)
    {
        MPI_Aint extent;
    	MPI_Type_contiguous(size, MPI_INT, &block);
        MPI_Type_commit(&block);
#if defined(PARSEC_HAVE_MPI_20)
        MPI_Aint lb = 0; 
        MPI_Type_get_extent(block, &lb, &extent);
#else
        MPI_Type_extent(block, &extent);
#endif  /* defined(PARSEC_HAVE_MPI_20) */
        parsec_arena_construct(o->arenas[PARSEC_rtt_DEFAULT_ARENA],
                               extent, PARSEC_ARENA_ALIGNMENT_SSE,
                               block);
    }
#endif

    return (parsec_handle_t*)o;
}
}}}

Then, this is when the {{{parsec_rtt_object_t}}} preparation should be
completed, to construct the arena that will potentially allocate
temporary data. There is one arena per data type that was expressed in
the JDF, and a /default/ arena. If all flows are typed, the default
arena does not need to be constructured. An arena is defined by the
number of bytes to store elements ({{{size}}} characters in this case), the
alignment of the data chunks (this can be necessary when using GPU, to
receive data in memory that is pinable for example), and the /MPI/
datatype used to transfer such data. In this case, we create a vector
type that holds one block of {{{size}}} bytes aligned together.

On a shared memory system, where /MPI/ is not needed, it is not
necessary to construct the arenas.

{{{
/**
 * @param [INOUT] o the parsec object to destroy
 */
void rtt_destroy(parsec_handle_t *o)
{
#if defined(PARSEC_HAVE_MPI)
    MPI_Type_free( &block );
#endif

    PARSEC_INTERNAL_HANDLE_DESTRUCT(o);
}
}}}

The destructor simply releases the MPI datatype and destroys the PaRSEC
handle.

=== Data Distribution ===

Now, the next step is to write a set of functions to access the data
and describe its distribution to PaRSEC. This is done in the file
{{{<SOURCE_DIR>/tests/pingpong/rtt_data.c}}}, that is explained below:
{{{
#include "rtt_data.h"
#include <stdarg.h>
#include "parsec/data_distribution.h"
#include "parsec/data_internal.h"
#include "parsec/debug.h"

#include <assert.h>

typedef struct {
    parsec_ddesc_t        super;
    size_t                size;
    struct parsec_data_s *data;
    uint8_t              *ptr;
} my_datatype_t;
}}}

The internal representation of the data {{{A}}} in the {{{rtt}}} JDF is
encapsulated in the {{{my_datatype_t}}} structure. It must derive the
initial datatype {{{parsec_ddesc_t}}}. This data distribution descriptor
will hold a single data block per /MPI/ process in {{{ptr}}}, of size
{{{size}}} bytes. However, PaRSEC does not manipulate directly the user's
memory (which is represented here by {{{ptr}}}), but meta-data, under the
form of {{{parsec_data_t*}}}, and {{{parsec_data_copy_t*}}}. A
{{{parsec_data_t}}} represents a user data at the abstract level, and the
{{{parsec_ddesc_t}}} class instanciates at most one per /MPI/ process per user
data element. A {{{parsec_data_copy_t}}} represents a copy, in a specific
revision, of the user data, and multiple instances can cohexist.
Distributed Data Descriptors usually manipulate {{{parsec_data_t}}} objects,
and provide {{{parsec_data_copy_t}}} objects for some interfaces, using
PaRSEC functions to instanciate them.

{{{
parsec_ddesc_t *create_and_distribute_data(int rank, int world, int size, int seg)
{
    my_datatype_t *m = (my_datatype_t*)calloc(1, sizeof(my_datatype_t));
    parsec_ddesc_t *d = &(m->super);

    d->myrank = rank;
    d->nodes  = world;
    d->rank_of     = rank_of;
    d->rank_of_key = rank_of_key;
    d->data_of     = data_of;
    d->data_of_key = data_of_key;
    d->vpid_of     = vpid_of;
    d->vpid_of_key = vpid_of_key;
    d->data_key    = data_key;

#if defined(PARSEC_PROF_TRACE)
    asprintf(&d->key_dim, "(%d)", size);
    d->key_base = strdup("A");
#endif

    m->size = size;
    m->data = NULL;
    m->ptr  = (uint8_t*)calloc(size, 1);

    return d;
}
}}}

PaRSEC will use all fields of the {{{parsec_ddesc_t}}} object, and it
is the user responsibility to fill them. {{{myrank}}} and {{{nodes}}}
are (respectfully) the rank of the local /MPI/ process and the number
of nodes in the /MPI/ Communicator. Then, the other obligatory fields
are function pointers to locate which rank holds which data, on what
Virtual Process (domain) this data must be bound, where the data is
for the local MPI process, how to compute a unique id for the data
from its multidimensional index, and if tracing capabilities are
enabled, a function to identify uniquely the data element. Those are
given by (respectfully) the functions {{{rank_of}}}/{{{rank_of_key}}},
{{{vpid_of}}}/{{{vpid_of_key}}}, {{{data_of}}}/{{{data_of_key}}}, and
{{{data_key}}}. The profiling system, if enabled, uses {{{key_dim}}} and
{{{key_base}}} to provide a user-readable representation of the data.
{{{key_dim}}} is expected to provide the range of the multidimensional
index of the data, while {{{key_base}}} provides how to prefix this
data (those are usually set in the user' program).

These fields are necessary to define the {{{parsec_ddesc_t}}} parent
object.  The {{{my_datatype_t}}} structure only adds to it a
placeholder to store the base pointer of the local part of the
data. This could be optional, since in the RTT benchmark, only one
data is used. But it is a good practice to have it relative to the
descriptor object, to facilitate reuse and multiple instanciations of
the same PaRSEC object.

The RTT benchmark will initially take the data from the rank 0, and
move it to the next placeholder until it reaches the end. Thus, each
rank needs only one data holder of size {{{size}}}, and that is why we
allocate only one. A more typical data descriptor would use the index to
define each local element.

The code for the access functions is given below:
{{{
static uint32_t rank_of(parsec_ddesc_t *desc, ...)
{
    int k;
    va_list ap;
    my_datatype_t *dat = (my_datatype_t*)desc;

    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);

    assert( (unsigned int)k < dat->super.nodes && k >= 0 );
    (void)dat;
    return k;
}

static uint32_t rank_of_key(parsec_ddesc_t *desc, parsec_data_key_t key)
{
    va_list ap;
    my_datatype_t *dat = (my_datatype_t*)desc;

    assert( (unsigned int)key < dat->super.nodes && (unsigned int)key >= 0 );
    (void)dat;
    return (uint32_t)key;
}
}}}

{{{rank_of}}}/{{{rank_of_key}}} returns the rank of a given data element,
based on its multidimensional index or its flat key. These functions must
return the same value on any /MPI/ process that call them, even if the
data location is remote.

{{{
static int32_t vpid_of(parsec_ddesc_t *desc, ...)
{
    int k;
    va_list ap;
    my_datatype_t *dat = (my_datatype_t*)desc;

    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);

    assert( (unsigned int)k < dat->super.nodes && k >= 0 );
    (void)dat; (void)k;
    return 0;
}

static int32_t vpid_of_key(parsec_ddesc_t *desc, parsec_data_key_t key)
{
   (void)desc;
   (void)key;
   return 0;
}
}}}

{{{vpid_of}}}/{{{vpid_of_key}}} returns the identifier of the Virtual Process
that "owns" a data identified by its multidimensional index or its flat key.
These functions must be defined only on the rank that hosts the data. The VPID
is a number between 0 and {{{vp_map_get_nb_vp()-1}}}.

{{{
static parsec_data_t* data_of(parsec_ddesc_t *desc, ...)
{
    int k;
    va_list ap;
    my_datatype_t *dat = (my_datatype_t*)desc;

    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);

    assert( (unsigned int)k < dat->super.nodes && k >= 0 );
    return parsec_data_create( &dat->data, desc, k, dat->ptr, dat->size );
}

static parsec_data_t* data_of_key(parsec_ddesc_t *desc, parsec_data_key_t key)
{
    return parsec_data_create( &dat->data, desc, key, dat->ptr, dat->size );
}
}}}

{{{data_of}}}/{{{data_of_key}}} returns the {{{parsec_data_t*}}}
associated with the user data at this multidimensional index position,
or for this flat key. These functions must be defined only on the rank
that hosts the data. The best way is to use {{{parsec_data_create}}} on
a persistent {{{parsec_data_t*}}} holder: this function will create the
data and assign it to {{{ptr}}} for that {{{desc}}}, {{{key}}}, and {{{size}}}
unless the holder already stores a {{{parsec_data_t*}}}. This function
is atomic and can be called by multiple threads.

{{{
static uint32_t data_key(parsec_ddesc_t *desc, ...)
{
    int k;
    va_list ap;
    my_datatype_t *dat = (my_datatype_t*)desc;

    va_start(ap, desc);
    k = va_arg(ap, int);
    va_end(ap);

    assert( k < dat->size && k >= 0 ); (void)dat;

    return (uint32_t)k;
}
#endif
}}}

{{{data_key}}} is used to convert a multidimensional
index identifier into a flat key.

When in the JDF the user wrote {{{:A(k)}}} to say that this task was
going to run at the same place as the data {{{A(k)}}}, PaRSEC will
call {{{rank_of(A, k)}}} (which is the multidimensional index
identifier) to compute what is the rank that holds the data. Here, the
code places this data on rank {{{k}}}, which is defined between 0 and
{{{world_size-1}}}, implementing a simple distribution of the
data. The parameters of these functions must be variadic arguments to
allow the user to define data on as many dimensions as wanted, and the
user must be careful to access the right number of arguments using the
va_arg macros.

When the user says that in a given flow, data comes from {{{A(k)}}} or
goes to {{{A(k)}}}, PaRSEC will call the {{{data_of(A, k)}}}, or the
{{{data_of_key(A, A->data_key(A, k))}}} function to obtain the
corresponding {{{parsec_data_copy_t*}}}. 

{{{
void free_data(parsec_ddesc_t *d)
{
    my_datatype_t *m = (my_datatype_t*)d;
    if(NULL != m->data) {
        parsec_data_destroy( m->data );
    }
    free(m->ptr);
    m->ptr = NULL;
    parsec_ddesc_destroy(d);
    free(d);
}
}}}

The last part of this file consists in releasing allocated resources.

The user is encourage to re-use the data distributions that have
already been coded in the data_dist/ subdirectory of the parsec source
code, instead of creating her own for every occasion. However, this
might be necessary for specific purposes.

=== Main Programm ==

Once this is completed, we can easily code the main RTT benchmark:

{{{
#include "parsec.h"
#include "rtt_wrapper.h"
#include "rtt_data.h"

int main(int argc, char *argv[])
{
    parsec_context_t* parsec;
    int rank, world, cores;
    int size, nb;
    parsec_ddesc_t *ddescA;
    parsec_object_t *rtt;

#if defined(HAVE_MPI)
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &world);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
#else
    world = 1;
    rank = 0;
#endif
    cores = 8;
    parsec = parsec_init(cores, &argc, &argv);
}}}

First, the MPI level must be initialized if necessary. Then, parsec
must also be initialized to create the parsec_context_t, and specify
the number of cores that one wants to use for parsec-related operations.

{{{
    size = 256;
    nb   = 4 * world;

    ddescA = create_and_distribute_data(rank, world, size);
    parsec_ddesc_set_key(ddescA, "A");
}}}

Then, the data must be created and distributed. The user is encouraged
to re-use the data distributions that have already been coded in
parsec/data_distribution, or create her own ad-hoc data distributions,
as is the case for the rtt benchmark.

Data keys can be put to help profiling the execution of the PaRSEC operation.
    
{{{
    nb   = 4 * world;
    rtt = rtt_new(ddescA, size, nb);
}}}

Then, the DAG generator is instanciated, with the corresponding data
description and additional parameters. At this time, PaRSEC computes
the initial tasks for this DAG with these parameters, and the number
of tasks to run locally.

{{{
    parsec_enqueue(parsec, rtt);
}}}

It is enqueued to the parsec context.

{{{
    parsec_context_start(parsec);
    parsec_wait(parsec);
}}}

The PaRSEC context is started (computing threads exit their passive
waiting loop, and start computing the rtt), and the main thread joins
them in {{{parsec_wait}}}. This blocks until the PaRSEC enabled
operations that have been enqueued in the PaRSEC context complete.

{{{
    parsec_handle_free((parsec_handle_t*)rtt);

    free_data(ddescA);

    parsec_fini(&parsec);

#ifdef PARSEC_HAVE_MPI
    MPI_Finalize();
#endif

    return 0;
}
}}}

When this is done, the parsec engine can be release, the data
descriptors freed (if the PaRSEC enabled operation had a side effect on
the data, which is usually the case, results should be read in the
data before freeing the data descriptors), and finally the MPI level
can be finalized.
